---
---
@inproceedings{Wang_2021_CVPR,
    author    = {Wang, Shipeng and Li, Xiaorong and Sun, Jian and Xu, Zongben},
    title     = {Training Networks in Null Space of Feature Covariance for Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    year      = {2021},
    pages     = {184-193},
    selected = {true},
    code = {https://github.com/ShipengWang/Adam-NSCL},
    abbr = {CVPR},
    award = {Oral},
    honor = {Oral Presentation [Top \%4]},
    pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Training_Networks_in_Null_Space_of_Feature_Covariance_for_Continual_CVPR_2021_paper.pdf},
    abstract = {In the setting of continual learning, a network is trained on a sequence of tasks, and suffers from catastrophic forgetting. To balance plasticity and stability of network in continual learning, in this paper, we propose a novel network training algorithm called Adam-NSCL, which sequentially optimizes network parameters in the null space of previous tasks. We first propose two mathematical conditions respectively for achieving network stability and plasticity in continual learning. Based on them, the network training for sequential tasks can be simply achieved by projecting the candidate parameter update into the approximate null space of all previous tasks in the network training process, where the candidate parameter update can be generated by Adam. The approximate null space can be derived by applying singular value decomposition to the uncentered covariance matrix of all input features of previous tasks for each linear layer. For efficiency, the uncentered covariance matrix can be incrementally computed after learning each task. We also empirically verify the rationality of the approximate null space at each linear layer. We apply our approach to training networks for continual learning on benchmark datasets of CIFAR-100 and TinyImageNet, and the results suggest that the proposed approach outperforms or matches the state-ot-the-art continual learning approaches.}
}
@article{
    Wang_Sun_Xu_2019, 
    title={HyperAdam: A Learnable Task-Adaptive Adam for Network Training}, 
    volume={33}, 
    abstract={Deep neural networks are traditionally trained using humandesigned stochastic optimization algorithms, such as SGD and Adam. Recently, the approach of learning to optimize network parameters has emerged as a promising research topic. However, these learned black-box optimizers sometimes do not fully utilize the experience in human-designed optimizers, therefore have limitation in generalization ability. In this paper, a new optimizer, dubbed as &lt;em&gt;HyperAdam&lt;/em&gt;, is proposed that combines the idea of “learning to optimize” and traditional Adam optimizer. Given a network for training, its parameter update in each iteration generated by HyperAdam is an adaptive combination of multiple updates generated by Adam with varying decay rates . The combination weights and decay rates in HyperAdam are adaptively learned depending on the task. HyperAdam is modeled as a recurrent neural network with AdamCell, WeightCell and StateCell. It is justified to be state-of-the-art for various network training, such as multilayer perceptron, CNN and LSTM.&lt;/p&gt;}, number={01}, journal={Proceedings of the AAAI Conference on Artificial Intelligence}, 
    author={Wang, Shipeng and Sun, Jian and Xu, Zongben}, 
    year={2019}, 
    pages={5297-5304},
    selected = {true},
    code = {https://github.com/ShipengWang/HyperAdam},
    abbr = {AAAI},
    award = {Poster Spotlight},
    honor = {Poster Spotlight}, 
    pdf = {https://ojs.aaai.org/index.php/AAAI/article/view/4466}
    }

@ARTICLE{VR_2022_Wang,
  author={Wang, Shipeng and Yang, Yan and Sun, Jian and Xu, Zongben},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Variational HyperAdam: A Meta-Learning Approach to Network Training}, 
  year={2022},
  volume={44},
  number={8},
  pages={4469-4484},
  pdf = {https://ieeexplore.ieee.org/abstract/document/9361276},
  code = {https://github.com/ShipengWang/Variational-HyperAdam},
  abbr = {TPAMI},
  selected = {true},
  abstract = {Stochastic optimization algorithms have been popular for training deep neural networks. Recently, there emerges a new approach of learning-based optimizer, which has achieved promising performance for training neural networks. However, these black-box learning-based optimizers do not fully take advantage of the experience in human-designed optimizers and heavily rely on learning from meta-training tasks, therefore have limited generalization ability. In this paper, we propose a novel optimizer, dubbed as Variational HyperAdam, which is based on a parametric generalized Adam algorithm, i.e., HyperAdam, in a variational framework. With Variational HyperAdam as optimizer for training neural network, the parameter update vector of the neural network at each training step is considered as random variable, whose approximate posterior distribution given the training data and current network parameter vector is predicted by Variational HyperAdam. The parameter update vector for network training is sampled from this approximate posterior distribution. Specifically, in Variational HyperAdam, we design a learnable generalized Adam algorithm for estimating expectation, paired with a VarBlock for estimating the variance of the approximate posterior distribution of parameter update vector. The Variational HyperAdam is learned in a meta-learning approach with meta-training loss derived by variational inference. Experiments verify that the learned Variational HyperAdam achieved state-of-the-art network training performance for various types of networks on different datasets, such as multilayer perceptron, CNN, LSTM and ResNet.}
  }



inproceedings{song2023consistency,
  title={Consistency Models},
  author={Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
  booktitle={the 40th International Conference on Machine Learning, 2023.},
  abbr={ICML},
  pdf={https://arxiv.org/abs/2303.01469},
  code={https://github.com/openai/consistency_models},
  media={https://techcrunch.com/2023/04/12/openai-looks-beyond-diffusion-with-consistency-based-image-generator/},
  year={2023},
  poster={ICML2023/consistency.pdf},
  selected={true}
}

article{yang2022diffusion,
  title={Diffusion Models: A Comprehensive Survey of Methods and Applications},
  author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  journal={ACM Computing Surveys},
  abbr={ACM},
  abstract={},
  code={https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
  pdf={https://arxiv.org/abs/2209.00796},
  year={2022}
}
