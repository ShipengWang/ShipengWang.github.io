---
---
@inproceedings{Wang_2021_CVPR,
    author    = {Wang, Shipeng and Li, Xiaorong and Sun, Jian and Xu, Zongben},
    title     = {Training Networks in Null Space of Feature Covariance for Continual Learning},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2021},
    pages     = {184-193},
    selected = {true},
    code = {https://github.com/ShipengWang/Adam-NSCL},
    abbr = {CVPR},
    award = {Oral},
    honor = {Oral Presentation [Top \%4]},
    pdf = {https://openaccess.thecvf.com/content/CVPR2021/papers/Wang_Training_Networks_in_Null_Space_of_Feature_Covariance_for_Continual_CVPR_2021_paper.pdf},
    abstract = {In the setting of continual learning, a network is trained on a sequence of tasks, and suffers from catastrophic forgetting. To balance plasticity and stability of network in continual learning, in this paper, we propose a novel network training algorithm called Adam-NSCL, which sequentially optimizes network parameters in the null space of previous tasks. We first propose two mathematical conditions respectively for achieving network stability and plasticity in continual learning. Based on them, the network training for sequential tasks can be simply achieved by projecting the candidate parameter update into the approximate null space of all previous tasks in the network training process, where the candidate parameter update can be generated by Adam. The approximate null space can be derived by applying singular value decomposition to the uncentered covariance matrix of all input features of previous tasks for each linear layer. For efficiency, the uncentered covariance matrix can be incrementally computed after learning each task. We also empirically verify the rationality of the approximate null space at each linear layer. We apply our approach to training networks for continual learning on benchmark datasets of CIFAR-100 and TinyImageNet, and the results suggest that the proposed approach outperforms or matches the state-ot-the-art continual learning approaches.}
}

inproceedings{song2023consistency,
  title={Consistency Models},
  author={Yang Song and Prafulla Dhariwal and Mark Chen and Ilya Sutskever},
  booktitle={the 40th International Conference on Machine Learning, 2023.},
  abbr={ICML},
  pdf={https://arxiv.org/abs/2303.01469},
  code={https://github.com/openai/consistency_models},
  media={https://techcrunch.com/2023/04/12/openai-looks-beyond-diffusion-with-consistency-based-image-generator/},
  year={2023},
  poster={ICML2023/consistency.pdf},
  selected={true}
}

article{yang2022diffusion,
  title={Diffusion Models: A Comprehensive Survey of Methods and Applications},
  author={Yang, Ling and Zhang, Zhilong and Song, Yang and Hong, Shenda and Xu, Runsheng and Zhao, Yue and Zhang, Wentao and Cui, Bin and Yang, Ming-Hsuan},
  journal={ACM Computing Surveys},
  abbr={ACM},
  abstract={},
  code={https://github.com/YangLing0818/Diffusion-Models-Papers-Survey-Taxonomy},
  pdf={https://arxiv.org/abs/2209.00796},
  year={2022}
}
